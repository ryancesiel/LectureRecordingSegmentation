[[Thinking Traps: Functional Fixedness, Problem Solving, and Creativity]]
PROFESSOR: In a way, I would say most of this course up until today, or maybe the memory lecture, was about how brilliant the human mind and brain is. How the visual system is so brilliant nobody can make a machine that sees so well. We didn't talk too much about physical action. But how we move in the world, how we act upon the world, there's no robot that can compete with you. Language is far beyond computers to be nearly as sophisticated as you are, as a five-year-old is.
So in all those domains, we mostly think about in terms of psychology or brain function, how is it that we're endowed with such amazing capacities? When it comes to thinking, one of the highest forms of human activity, thinking, hard problem solving, edges of creativity, that kinds of thing, it's something we have a huge respect for, something we wish for, something that we know is a very, very valuable tool.
But psychologists have emphasized the traps we can fall in. I mean in a way what's easy for us is what the brain empowers in us. It's easy for us to recognize objects or faces or move in the world. When we feel something is hard, like thinking hard, for problem sets or global warming or other things that are hard to think about, then we know that we're sort of leaving the apparatus that's easy for us and we're entering into a hazardous, difficult form of our mental lives.
So, I'm going to show you a video for a moment. Because the other fun thing is, and maybe this is true of all of us in different ways, all the time, we often don't recognize, we often don't recognize when we're thinking not as well as we might. So this is a trap.
So I'm going to show you, the original punking TV show, this was called Candid Camera. It's pretty old. So this is a clip from there. And here's a couple people asked to do a school problem, 7/8 divided by 3/4 and how many square feet in a square yard? And what's kind of funny about it in my mind is, not only that the people are wrong, but how wrong they are and how much they don't realize sometimes how wrong they are.
I mean what's impressive about this is that not only are the people wrong, but they don't say like, I forget. They just start to give answers, throwing out numbers. And so how different are all of us in that?
So some of these again are on the internet. And this is an MIT audience, which is about the hardest audience to pull this off. And at Stanford, I could pull this off much better. So some of these won't go as easily as they might for other typical audiences.
Here we go. Two train stations are a hundred miles apart. At 2:00 PM one Saturday afternoon, the two trains start toward each other, one from each station. One travels at 60 miles an hour, the other at 40 miles an hour. Just as the trains pulls out of their stations, a bird springs into the air in front of the first train, and flies ahead to the front of the second. When the bird reaches the second, it turns back, without losing any speed, and flies directly toward the first step.
Doesn't this remind you of all the high school physics problems you had earlier? Where's the frictionless pulley? The bird continues to fly back and forth between the trains at 80 miles an hour. How many miles will the bird have flown before the trains meet?
It's a little bit of a trick. Because the answer is told to you directly, pretty much. So 100 miles apart. How long will it take for the trains to reach each other, if one is 60 miles an hour and one is 40 miles an hour?
AUDIENCE: Hour.
PROFESSOR: Hour. So how many miles an hour does the bird fly?
AUDIENCE: 80.
Easier than you think. So those are the kinds of ones I just showed that people kind of get themselves into their thinking cap. And it's a trick.
Here we go. I need somebody who is willing to, at their chair, just call out words. Thank you very much.
Here we go. In English, many words are pronounced somewhat differently depending on the way they are spelled. But here we go.
Can you give me a synonym for people or family, a word that people would often say for that? I'll give you one. Folk.
Now, I know this is a history quiz. But I'll help you. I wouldn't know the answer to this, necessarily.
Can you give me the name of an American president at the time of the Mexican War? And I'll give you a hint. It rhymes with folk.
AUDIENCE: Polk.
PROFESSOR: Polk. Everybody can help. Everybody help. One, two, three. Can you give me the name of the word that means egg white?
AUDIENCE: Yolk.
PROFESSOR: Aha. No. Is the yolk, the egg white? I know. It's a pretty hilarious course. Thank you.
So this is an example of what people call functional fixedness. Your mind is going a certain way. And you're on a certain pace. And all of a sudden when you need to put the mental break on, your habit of thought takes you a slightly wrong direction, on a very superficial test. Thank you very much.
Here's an experiment. In 10 minutes, only about 39% of people solve this. You go into a room like this. The strings are too far apart. You have stuff around here. And your job to tie the strings together. What do you do?
AUDIENCE: Stand on the chair?
PROFESSOR: Standing on the chair is an interesting one. Let's pretend it doesn't work. It wouldn't. It would get you up, but they'd be far apart still. It could. Let's pretend it doesn't.
You'd have to be there, to be convinced. You know what I mean. It mean depends on the angle.
AUDIENCE: [INAUDIBLE].
PROFESSOR: Sorry?
AUDIENCE: Tie it to the end.
PROFESSOR: Ah. So you're one of the 39%. So the idea is, if you tie for example, the pliers onto the thing and get it swinging, you could catch it, right? So why don't people come up with that answer very easily?
PROFESSOR: Because what are pliers for? Pliering. They're not for swinging ropes. So again, the word they use is functional fixedness. You know what a plier is for. It doesn't seem like a good candidate to do this job.
Here's another one. You walk into a room and there's a door that's closed behind you. And you see these things on a table. And you're asked, could you support this candle on the door? About 23% of people get this, in 1945. What do you do? This is just like all the spy movies. Yes?
AUDIENCE: [INAUDIBLE].
PROFESSOR: Sorry?
AUDIENCE: Put the tacks through the width of the door.
PROFESSOR: Yeah, through the width of door would be pretty good. It's not really supporting the candle, but it could get the job done. That's not a bad thought.
These are all good thoughts. If it were me, I wouldn't be close. So that's better than I do. Yeah?
AUDIENCE: Take out the tacks and then tack the base.
PROFESSOR: Have you see this before? No. Excellent. As I tell you, MIT's audience is not an easy audience. The idea is that you dump the tacks, and then stick the tack into the back of the empty box. Nice platform for a candle.
Why don't most people think about that? Because what's the job of the box? To hold the tacks. My job is to figure out this problem. You hang on to the tacks.
And in fact if they do the same experiment, but just put the tacks next to the box, a lot of people get it right away. Because now that box, it's yearning to support something. It no longer has a job. It's an unemployed box that you can stick onto the door and support this. Your mind just gets stuck with thinking the box is a holder. It's not a platform.
This is the original-- and you've seen this so many times by now. This is one of the original, thinking outside of the box things, where you're supposed to connect the dots with four straight lines without lifting a pencil.
Some of you know this. Some of you are figuring out on the spot. But I'll just show you. So there's an answer. The other ones are similar.
Why do so many people get stuck in this and not get it done? Because they don't like this part. And they don't like this part, because it's drawing outside the box. They're always looking for the connections within the box. And it seems cheating in some sense to draw outside the box. There's an intuition, like with the box with the tacks, that you're supposed to stay in there.
So if they get no information in this study, practically nobody solved it, in a given time. If they get a hint, you can draw outside the square, that helps. But then, outside the square puts again, the first line. They're still only half the time getting it. Because you've got to go outside a second time. So it gets a lot of information to get comfortable with finding a way to do this that's completely within the rules, but not the way you intuitively feel might be the right way to do it.
So those are all examples of functional fixedness, where you have a certain belief about what's going on and it's hard to overcome that to solve a problem in a novel way. Another thing that people think about as the way to overcome some of these problems of thinking about problems is different representations that let you take a different look at a problem to be solved.
So picture a large piece of paper, a hundredth of an inch thick. Now, in your imagination, fold it once. You have two layers.
Continue to fold it on itself 50 times. It's true that's it's impossible to fold it in place, such a piece of paper, 50 times. But for the sake of the problem, imagining that you can, about how thick would the 50-times-folded paper be? And everybody's first intuition is, well, how thick can a piece of paper be? Of course, this is a hypothetical problem, not an actual paper problem. But if you do the math, it's big.
So here's another one. One morning, exactly at sunrise, a Buddhist monk began to climb a tall mountain. A narrow path no more than a foot or two wide, spiraled around the mountain to a glittering temple at the summit. The monk ascended at various rates of speed, stopping many times along the way to rest and eat dried fruit he carried with him.
He reached the temple shortly before sunset. After several days of fasting and meditation, he began his journey back along the same path, starting it at sunrise and again walking at variable speeds, with many pauses along the way. His average speed descending was, of course, greater than his average climbing speed.
Show that there's a spot along the path that the monk will occupy on both trips at precisely the same time of day. So that sounds moderately hard. But if you draw the picture, so we went and get an equation. Now, we have a pictorial representation.
You can see there has to be some moment of the ascending and descending path that cross in time. You don't know what it's going to be. But there has to be some moment in time. And maybe that's obvious. But for a lot of people it seems stunningly complicated that you could even demonstrate that in any way at all.
Here's one you could think about. Suppose you are a doctor faced with a patient who has a malignant tumor in his stomach. It's impossible to operate on the tumor, but unless the tumor is destroyed the patient will die.
There's a kind of ray that can be used to destroy the tumor. If the rays reach the tumor all at once, with sufficiently high intensity, the tumor will be destroyed. Unfortunately, at this high intensity the healthy tissue that the rays pass through, will also be destroyed.
At lower intensity, the rays are harmless to healthy tissue, but they don't affect the tumor either. What type of procedure might be used to destroy the tumor with the rays, and at the same time avoid destroying the healthy tissue?
Now, if you know the answer to this, don't jump right up. Give other people a few moments to puzzle. About 10% of people solve this in a small unit of time, before the Internet. And so, what's the answer though, for those of you? Yeah, I see some hands, I think. Go ahead. Did you want to say?
AUDIENCE: [INAUDIBLE].
PROFESSOR: No. Exactly at the spot, sir.
AUDIENCE: [INAUDIBLE].
PROFESSOR: Yeah. Have several weak ones aimed to converge at the tumor. I saw your hands.
So the idea is to have several different rays, each of which are weak enough not to destroy the tissue. But when they converge spatially at the tumor, they would sum enough to destroy the tumor. That's the problem-solving answer to this.
So knowing that, here's another kind of a problem. A dictator ruled a small country from a fortress. The fortress was situated in the middle of the country and many roads radiated outward from it, like spokes in a wheel. A great general vowed to capture the fortress and free the country of the dictator.
The general knew that if the entire army could attack the fortress at once, it could be captured. But a spy reported that the dictator had planted mines on each of the roads. The mines were set so that small bodies of men could pass over them safely, since dictator needed to be able to move troops and workers about.
However, any large force would detonate the mines. Not only would this blow up the road, but the dictator would destroy many villages in retaliation. A full-scale, direct attack on the fortress seemed impossible.
Does this problem remind you of the other problem, I mean back-to-back, like this? He divided his army up into small groups. Now, they're small enough per path into the castle. And they converge on the castle, like the X-rays converged on the tumor.
So this is reasoning by analogy. We have problem solving approach. Divide your resources spatially. Have them meet at the same time, and converge in a way that's effective.
So what you might call the deep structure of the problem is the same, with the surface story being different. One is tumors and treatments. But there's a strict analogy.
Capturing the castle is like destroying the tumor. The fortress is like the tumor that you want to get to. The army is like the rays and the general is like the doctor.
So if it is back-to-back like this, where I just said, here's a problem. And here's a problem just like that. Everybody says, well, send the troops on the different paths and have them converge at the same time. If you don't say that though, if you don't tell people this is the same, use the same rule. If you don't tell people that, only about a third come up with it spontaneously.
If you just make a little bit of an effort not to make it completely obvious that it's the same problem. You just wait a little bit. So the surface differences, the war story versus the medical story, will mostly throw people from recognizing the analogies.
People recognize surface analogies all the time. If we have war in the Middle East now, from the US perspective there's analogies to Vietnam, a war and a war. But other kinds of analogies across situations, people have a hard time mapping, if the surfaces look different.
And here's an example where they gave people two problems of a certain kind. And they said as part of the experiment, and here's a classroom demonstration. If it's immediate, they sort of get it. But if they wait just a little bit, and they move from one room to another room, they don't apply the rule at all.
So it's very tricky. We would like to think well, gee, we know all kinds of things about the world, that we can bring in a mental toolkit to solve problems, all kinds of ideas. But if the problem doesn't look the same as the prior problem, people have a hard time spontaneously saying, there's an answer I know, and I can apply it to the situation, unless it's really, really obvious. People don't transfer deep solutions very easily across what looks like different situations.

[[Judgement and Decision Making: Heuristics]]
Something different. The instructors in a flight school adopted a policy of consistent, positive reinforcement, recommended by psychologists. And we know we're very suspicious about psychologists, right? But you have to imagine the instructor went to some classroom and somebody said, positive reinforcement is the way to go.
They verbally reinforced each successful execution of a flight maneuver. After some experience with this training approach, the instructors claimed that contrary to psychological doctrine, high praise for good execution of complex maneuvers typically result in a decrement of performance on the next try. Are they correct?
So you understand, you have to have in your mind all the military movies where there's a really tough sergeant training. The other one that yells at you to make you succeed. But for half the movie, you don't know if the sergeant will break you or make you into a better woman or a better man.
So he's barking orders. And some psychologist says, well, you should really think about saying nice things to these people, because that's encouraging. Errgh. Somebody does a flight maneuver and does a really good job. And he goes, that's awesome. And then the person goes up and does worse. And he goes aargh, you're all terrible. So what's happening? Yeah.
AUDIENCE: It must-- the person typically does really well. Doing really well one time isn't indicative of really well the next time.
PROFESSOR: It's even worse than that, right. So all of us have in us a range of performance. We know that. If you do sports, if you do research, if you do anything, some days we're at our average. Some days, we're not so good. Some days, we go wow, that was pretty good.
We all have a range of performance that's possible, our average performance, our best days, our worst days. So you just went up and you did an awesome flight maneuver. Is that your average one or is that a peak example?
It's a peak example. Statistically, what's the likelihood of your next one going to be, better or worse? Worse. You can't always be at your best, because you'd sort of go into infinite excellence.
You have a range of performance. In the NBA, what's a really good score? 30 points is really good.
That's the average though, of the best players. They don't go out and go, I got 30 tonight. And tomorrow, I'll get 32. And by the end of the season, I have like 180 points every game. Even if you're on the Miami Heat, you have to do a good job. So do you understand?
So here's another example. Parents of very famous, successful kids, are they mostly super-famous and super-successful? Or do we mostly read about them and go whoa, that's a tough deal, what happened there? What's your impression?
Let's ask this question-- and if you are facing these burdens. If your parent wins a Nobel Prize, what's the odds that you win a Nobel Prize. Small.
And they go oh, the underachieving kid. All the pressure from the parent to do really well, with a Nobel Laureate as your parent. Well, maybe. Maybe your parent is berating from kindergarten on saying, if you don't eat your peas and carrots, you won't win a Nobel Prize, like I. That's a possibility. We can't rule it out.
But if somebody in your family wins a Nobel Prize, what's the odds that other people in your family will win that.
PROFESSOR: Yeah. So we call that regression to the mean. If you have an outstanding example, a person who wins a Nobel Prize, everybody around them is not likely to win. If you have an outstanding day at what you do, like a really good day, the next day is likely to be worse, just statistically.
So what's happening with this flight instructor? This is a very powerful and important idea, that intuitively often, what's happening with the flight instructor? Of course if the guy does awesome, he's going to do worse the next time, every single time, practically, statistically. So we can think about this, regression to the mean.
Here's a standard distribution, bell-shaped curve. Here's the mean of whatever you do, how well you shoot baskets, how well you study for a test, how well you wrote a paper, how well you helped a friend who needed consoling. You pick your thing.
Some days you do a medium job. You're average. Some days you really impress yourself with how you did. And some days, you're pretty shocked at what a miserable day you had.
Does that makes sense? So if you had an awesome outing, by average, the next outing is going to be less good, regression to the mean.
So one place that this has been done over and over and over again, and somebody help me out here. This could happen in any sport. But especially in basketball, people talk about a hot hand. So some of you are sports enthusiasts. Probably some of you are not.
So a sports enthusiast, will you please help me describe what a hot hand is in basketball? Help the rest of the class. Oh, thanks.
AUDIENCE: You think you're on a hot streak just because you made a couple baskets in row. But they do the statistics and then it ends up being that they're just overconfident in their shots, since they are shooting more shots in the end.
PROFESSOR: So people have a saying -- If you watch a basketball game, it's incredibly how often the announcer will say, oh, player x, he or she is on a tear. They're so hot. Pass that ball to that person. They're so hot.
They shoot a three-pointer. And they come down and they shoot a three-pointer. And they say, why are you passing the ball to that person? They're so hot.
And some players are known as streaky players, who go out and have just awesome nights. You hear this in the sports all the time. I can't tell you, the end of a close March Madness game, they're always saying, pass it to this guy. Because this guy is on a streak in this game.
So they can do the statistics. How would you do the statistic to know whether streaks actually-- streaks exist. Sometimes you do two baskets in a row or five. Or you miss five in a row. There are runs, like heads and tails.
But the question we're asking is, if you made a shot, does that an increase in any short run, the probability that you'll make the next shot? That would be the streak. That would be the hot shooter.
So you can go do statistics. And they did it. Psychologists have done it on many teams, in many leagues. They did every single shot of a Cornell basketball team, the entire season. But they've done it on many other teams. There is no such thing as a hot streak. There is no such thing as a hot streak.
Of course, you have small runs, where people have a lot of good shots. Another day, they'll have a lot of bad shots. But the probability of making any one shot is no higher or lower, given the probability that you made the prior shot. They're completely independent.
And nobody can find statistical support for any kind of streaks in sports, except of course you have huge distributions. So sometimes a person will have an exceptionally good or bad night.
So this is kind of like that. And you will get this. Suppose that you have a normal penny with a head and a tail. You toss it six times. Which is these outcomes is most likely?
All the same. You're absolutely right. But why do people tend not to like a), mostly, who haven't studied probability much? They don't like a). Why don't they like a)? Most people, most of the time, as a random thing? Yeah?
AUDIENCE: It doesn't look random.
PROFESSOR: It doesn't look random, man. A) can't be random, right? But we know that we have short runs of anything. And each of these are independent outcomes. That's exactly what you got.
We did this example before. But there are 30 people in the group. You get the month and date of each person's birthday. What is the approximate probability that two people have the exact same birthday?
And most people will say, oh, I haven't met that many people on my birthday. Not that high. But it's actually 70%.
Because it's any two in the group. It's not you. It's any two in the group. And that increases it to 70%.
Here's one we haven't done. Imagine a college in which the average height of men is 175 centimeters. You randomly measure the height of three men. Which of these two outcomes is more likely? And I'll let you think about this for a moment.
John at 178, Mike at 170, and Bob at 176; or John at 177, Mike at 177, and Bob at 177? Which is more likely, or can't tell?
AUDIENCE: [INAUDIBLE].
PROFESSOR: I heard a B. Is it obvious? Let me tell you. It's the same concept we've been talking about, about distributions and means.
So here's the average, which you are given as 175. So these guys are closer to the average. Height is distributed in a normalized way.
So here's the three people, who are red dots, the equal heights, 177, and 177, and 177. Here are the heights of the blue. There's more people in this part of the distribution.
So it's actually more likely to get this than this, about 40% more likely. Because you're sampling from closer to the mean. And there are more people near the mean. Does that make sense?
But people don't like that. Overwhelmingly, if you ask people, they like the top answer, because it just doesn't feel right to get three people of equal height, in a random way. And you could say, what do you mean it doesn't feel right? We can do the math or we can show you the picture. And we can all agree, I think, that that's correct.
What do you mean it doesn't feel right? And that's what people mean by the difference between algorithms and heuristics. This is work from Kahneman and Tversky, that won a Noble Prize for Danny Kahneman. Tversky passed away.
Which is that humans could figure out many things algorithmically. You can do the probability. But as an intuition, for many, many, many people, even educated people who know a lot about probability.
There's an intuition that we have about things, heuristics, a feeling we have about things. Like the odds of this are almost nil. And we go by that feeling, instead of by being the rational analyst that we might be.
Here's one. You'll get this one, I think. A nearby town is served by two hospitals. About 45 babies are born each day in the larger, about 15 in the smaller. Approximately half of the babies are boys. However, the exact percentage varies day to day, of boys and girls.
Some days, it's higher than 50%. Some days, lower. For a period of a year, both the larger and the smaller hospital recorded the number of days in which more than 60% of the babies were boys. So a disproportionate number were boys.
Which hospital do you think recorded more such days? The answer is about the same. The most common answer is about the same.
But because of the law of small numbers, you're going to get more weird samples in the smaller hospital. Because it'll be a less of a distribution. You'll get more weird stuff.
Here's one. It's a little dated, in terms of the language used. But they would give people an example of this. Linda is a 31-year-old, single, outspoken, very bright. She majored in philosophy as a student. She was deeply concerned with the issue of discrimination and social justice and also participated in antinuclear demonstrations.
And then rank the options in terms of their likelihood. In describing Linda, I'll give a ranking of 1 to the most likely option and a ranking of 8 to the least likely option. So people go through this.
I mean you're sort of just making up stuff. Having said this, I think people go around all the time having impressions of people in their head, what are they really like, what are they really about. But they do this kind of thing.
And here's the interesting element to the result. People on average, are more likely to endorse the statement, Linda is a bank teller and active in the feminist movement, than Linda is a bank teller.
Now, why logically is that a bad idea, as you're guessing about Linda and what you might do? Why is it logically a bad idea to pick h, more than f?
AUDIENCE: [INAUDIBLE].
PROFESSOR: Right. Yeah. Yeah. Just logically, heuristically, algorithmically, right, sorry, it's more likely you'd be correct saying she's a bank teller. She's a bank teller and shes--
But why do you think more people pick this one? Because of your intuition. So people think, well this is a person who might be like this? So they go for the bigger description.
And not just unsuspecting undergraduates, but also first-year grad students in stats courses, and also doctoral students in decision science programs and business school. It's not, with more training you can avert these errors. But everybody has it in them pretty easily to be seduced by intuitive heuristics of reasoning, rather than algorithmic analyses.
Availability is kind of fun. So this is a slow one. This example is a little work, for an example.
Some experts studied the frequency of appearance of various letters in the English language. They studied a typical passage in English and recorded the relative frequency with which various letters of the alphabet appeared in the first and the third positions of words. For example, in the word "language," appears "L" appears in the first position and "N" appears in the third.
In this study, words with less than three letters were not examined. Consider the letter "K." Do you think that the K is more likely to appear in the first position or the third position? Now estimate the ratio for the number of times it appears in the first versus the third.
So it could be even, 1:1; it could be more often in the third position, like 1:2; or it could twice as often in the first position. Any feelings?
Just for fun, who thinks K appears more often in the first word than the third? Just for fun. Help me out here.
Hands up. If you have to pick one or the other. You have to pick one or the other. How many people like in the third position?
Anyway, so here's what happens mostly. Not your estimate, but the estimate of students in larger samples. 2:1, they like K in the first position. It's really 1:2 in the dictionary. Why do you think on average, people tend to go for K in the first position?
So now we need somebody who's willing to say something, give some answers out there. Yeah?
AUDIENCE: Because it's easier to think of words that start with K than it is--
PROFESSOR: Yes. So help me out. Let's do this for one second. Somebody call out a K word, any word?
AUDIENCE: Knight.
PROFESSOR: Knight. Another word?
AUDIENCE: Kind.
PROFESSOR: Kind. Another word?
AUDIENCE: Kangaroo.
PROFESSOR: Kangaroo. Excellent. OK, very good. OK, quick, a K in the third position?
AUDIENCE: [INAUDIBLE].
PROFESSOR: Oh, you guys are-- I should have done it differently.
So now we're talking about the heuristic of availability, that we sort of think that how common something is and then in some ways, how important something is, by the ease with which it comes to mind. It's easier really to think of words beginning with K, then having K in the third position.
Here's a question. How much more likely are you, and I don't know the answer to this, but you can figure it out, to drown in the bathtub than to die from a terrorist attack in the United States? Obviously since 9/11, terrorism has been a a huge issue in our county, a huge issue.
But in the US, what do you think? Much more likely for people to die drowning in a bathtub than terrorist attacks, including 9/11, for the 2000s.
So why is it, why is it that you have constant discussion about terrorism? Why is it that you take off your shoes and can't take much of your toothpaste on the airplane? And yet nobody is talking to you, saying, please exercise caution when you go into the bathroom. And please have those things, those sticky things that make it less likely to slip.
Do you get much of that? Do you go to the airport and they say, those shoes look a little slippery, in case you go into a shower.
So think about this, which is the real danger, just in a practical way, and why do people worry so much, and not for bad reasons, about terrorism? What's the difference between them? And you can think of more than one. Yeah?
AUDIENCE: You hear about terrorism on the news, but you don't hear about anyone drowning in the bathtub.
PROFESSOR: That's huge. Availability is huge. The news is a very powerful way in which we think that's what the world is full of, terrorism in the US. We hear about terror groups that are found, terror cells that are found. We'll hear about homicides a little bit, in the local news.
But you just never hear about, unless it unfortunately happens to somebody you know personally, somebody slipping and dying in the bathtub. And it does happen. But you don't hear about it much. It's not available to you. Yeah?
AUDIENCE: We're most scared of things that are foreign to us even if they are less dangerous. Like we'd be more likely to drink a clear fluid that was more dangerous than something that looks scary.
PROFESSOR: Part of it is, what you're saying, there's a sense of foreignness to the terrorism that we don't understand. Maybe there's a sense of passivity. We feel like we could do something about the bathroom. We can't do something about an airplane being driven into a building.
These things are complicated. But it's fascinating what people think is scary versus what by any rational analysis, is dangerous.
Here's another kind of way that people easily struggle with numbers and thinking about what's dangerous and not. So imagine you're a physician and you're presented with this information. And let's hypothesize that it's true.
Somebody comes in with dizziness. And they say, I'm worried about a brain tumor. They go to your neurologist.
And the neurologist says well, here we have some statistics over the last two years, whether somebody turned out to have a brain tumor or not, present or absent; and whether they were dizzy or not, present or absent. So you can make a 2 x 2 cell, about they had a tumor and they reported dizziness; they reported dizziness, but it turns out they had the tumor; and so on.
So here's the question for you. Is being dizzy something to worry about in terms of having a tumor? Should you be worried if you're dizzy, that you might have a brain tumor, by these numbers? So what do you think most people think? You have to make a quick answer.
You're one of those doctors on House, who's going to get berated in a moment. And you go, whoa, 160, bad. That's what a lot of people feel.
And by the way, there's a lot of concern in medicine about how doctors convey risk to patients, as patients pick what kind of treatment they want. Because it's exactly what we're talking about now. It's the exact way that you convey to a patient, the risk they're facing or their family member is facing among different treatment options.
So is dizziness associated with a brain tumor? And the answer is no, because if you do the probabilities, so the probability of a tumor is 4:1.
So here's whether you have the tumor, brain tumor. And it's one out of four, and it's one out of four. The odds are identical. But you're impressed by this absolute, big number. Intuitively, almost everybody is, almost all the time.
Here's another one that's a bit more subtle. It comes up all the time in medical testing. It's come up in things like discussions about how often we should test for, if you're a male, for prostate cancer when you get older; if you're a female, for breast cancer. So these kinds of things come up all the time in decisions about should you get the tests or not, and so on.
So imagine now, a hypothetical syndrome, is a serious condition that affects one person in 1,000. Imagine also that the test to diagnose the disease always indicates correctly that a person who has it, actually has it. So if you actually have the disorder and you get a yes, you will always be identified. So that's a good test in that way.
Finally, suppose the test occasionally misidentifies a healthy individual as having that disorder. So that's a false positive. The test is a false positive at a rate of 5%. Meaning the test wrongly indicates that the virus is present in 5% of cases where the person does not have the virus.
So if you have it, it's always correct. But 5% of healthy people will be misidentified on the initial test.
Choose a person at random, administer the test, and the person tests positive for that syndrome. What is the probability the person really has that? This is the kind of medical testing topic that goes on all the time, all the time. What do you think most people are going to say?
AUDIENCE: 95%.
PROFESSOR: Yeah, 95% exactly. Because he said well, 5% of the time it's wrong. But that ignores the base rate of this order. Because if there's 1/1000, then if the other 999 are tested, then you have about 0.5 times 999. That means for every 51 who are tested, only one has it. So the answer is only about 2%.
Because the odds, the prior odds, that you have it are so, so low. So that comes into the likelihood that it's an accurate test and the false positive rate. So again, people's intuition when they hear, their intuition is there's a 95% probability that the person has the disorder. It's really only about 2%, given the low known base rate.
And if all this is whizzing by you a bit, I know I'm going through it pretty fast. But you can think in two ways. You can also think you're a patient being told this. You're not going to get an hour long lecture on probabilities either. So how this is communicated to physicians, for example in the medical area and families, is a really big challenge.
So we talked about two heuristics, that people liked things to look random, that we tend to go for what we often hear about and imagine that's how frequent or dangerous something is, in a very broad, intuitive way.

[[Influencing Decisions: Anchoring, Adjustment, and FramingFrontal Lobes and Thinking]]
Anchoring and adjustment, here's a fun one. So here's the experiment they did. They went to people, like in courtyards around things that are around Stanford, actually mostly. And they had them do the following. They gave them either this problem or this problem. And here's what happened.
If they got this problem, and they quickly had to guess the answer, they came up with this many. This was their average estimate and this was the average estimate.
So the first thing we can agree upon, regardless of the fact that everybody is completely underestimating the actual answer-- that's almost not the point-- regardless of that, look at this estimate. This rapid estimate is four times greater than that. Why is that happening?
You're only getting one or the other. But why is that happening? This is the heuristic of anchoring, how people come to quick decisions.
If we go, 8, that's a pretty big number. 8, we're talking some big numbers here, right now. They don't even get that they're going to get to here.
They start with 1 and 2. They go, we're talking about a pretty small thing. But I can't quite calculate it out, because it's too hard to calculate it out for most of us.
This is going to be pretty small by the time-- so they are already thinking small. They are already thinking big.
So now you know why, when you go negotiate for your salary somewhere in the future, if you have any negotiating power, start with a big number. And the person who is negotiating with you knows that too, and in fact is often more practiced than you are.
So here's a practical clue. Start with a big number. Because that's the number where people's minds are.
And I'll show you several more examples of how ridiculous this is. But when there's no true, exact number that you can quickly come up with, what's the fair salary for you to get, the first year out of MIT in a job or something? First number out there, has a lot of power.
So here they made it ridiculous. This anchoring, that my mind start somewhere and I kind of stay in that neighborhood. I don't recalibrate myself.
So they took a wheel of fortune, a spinning wheel of fortune, with the numbers 1 to 100. It's clearly random. Can't make it more obviously random than that.
It stops somewhere. And they say, we want you to estimate the number of African nations in the UN? And most people don't really know. And there's some vague sense that it's more than a couple. But who knows how big it is exactly.
So the number stops somewhere. And here's the kind of thing that happens. First you say, is it more or less than the number it randomly stopped at?
Let's pretend it stopped at 65. Then average estimate was 45 African nations. Let's say it randomly stopped at 10. Then the average estimate was 25 nations. What's happening? Anchoring.
You say well, it's more than 10. But 10, I'm going to go way up from 10. Because that's way too low. And I'll split-- going beyond 25 seems hazardous.
You start at 65. You go, whoa, that's way too many. But you're anchored. And then you use don't go down that much. So that first number, when you don't really know what the right answer is, has an incredible power over how far you range from that, for your best estimate.
Same idea. Is the Mississippi River longer or shorter than 500 miles? So they're giving you the anchor. How long is it actually do you think?
Or another person is asked, is it longer or shorter than 5,000 miles? There's the anchor. How long is it, do you think?
And here's the estimates. They're both underestimating. But they're moving towards that number that was completely randomly thrown in. But it's not random to the human mind. We don't know what the right number is, but we're going to start in the neighborhood that we first have any number presented to us.
Framing. Framing is arguably, together with availability, maybe the one that's the most interesting in terms of how powerful it is and how often people use it to convince you that their political position, their corporate position, their anything, is the way to go. And once you know framing, you're empowered a bit to think about that, because you hear arguments made to you about what to believe in or what to support.
So here comes the framing. Imagining the United States is preparing for the outbreak of an unusual South American disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientific estimate of the consequences of the programs were as follows.
If Program A is adopted, 200 people will be saved. If Program B is adopted, there's a one-third probability the 600 people will be saved and a two-thirds probability that no people will be saved. Which program do you favor?
So B is pretty complicated. But A is pretty simple, 200 people might be saved, will be saved.
So here's the exact same scenario by lives and deaths, given to a second group of research participants. If Program A is adopted, 400 people will die. Do you see that if A is adopted, 200 people are saved, equals 400 people will die. It's the same thing. There's 600.
You're really saying the same outcome. But that numerical identity is treated by humans as incredibly different information on which to base your decision. And here's what happens.
If people are given this scenario, which is explained in terms of lives saved, 200 lives saved out of 600, 3/4 of the time they pick it. If people are given this scenario, 3/4 of the time they don't pick it. It's the exact same choice.
But to be human, is to gravitate towards something where you save lives and avoid something where you lose lives. Even though numerically, it's the exact same outcome. But people radically switch which one they think is the correct one, by the framing of it. By simply the way it's described in this most simple sense.
So does that makes sense? I mean it's very powerful. When people present to you stuff, how do they present it? Lives saved, lives lost; jobs made, jobs lost; whatever you want, money made, money lost.
We don't treat them as two bits of information that are equal. We should. That's the algorithm, out of 600 people, 200 people make it. But that's not how we psychologically interpret it. We go tremendously by, we're drawn-- any answer that uses the word "saves", is a positive thing. And we're averse from any answer that talks about bad things, how many people will die. Even though they're identical in the consequence of life and death.
Is that clear? Very powerful. Here's another version of it now, moving to money. And it gives you another feeling of this. So you could think about the top one.
Would you rather have a sure gain of $75. Somebody said, I'll give you $75 right now. Or you can have a lottery where you have a 75% chance of winning $100, so that's even more, or a 25% chance of winning nothing at all.
Another group of people get this question. We're going to take from you $75. It's a rough experiment to sign up for. Or you're going to be in a lottery with a 75% chance of losing $100, which is even a worse loss, and 25% chance of losing nothing at all.
So these are meant to be kind of symmetric stories. But one is expressed all in terms of gain and one all in terms of losses. And when people are thinking about gains, they're risk averse. I'm not going to give up what's in my hands.
So 84% of the time, they take that. Even though this is sort of formally similar because it's a loss, now when it's losses, people are risk taking. Humans are very asymmetric. If things are expressed in gains, we're risk averse. Let's do the safe thing. I'm going to hang on to what's already in my grasp.
If it's in terms of loss, it's like, let's bet the house, let's do any crazy, wild thing. Because losses are so bad that I'll take any chance to avoid a loss. Even though rationally, the choices should be about the same, if we were machines making choices.
Very big difference. I mean when I first heard about these things, I was stunned. Because it's huge. Everything that we vote for, everything that we support, every decision we make about careers, and policies that we are for or against, are always being expressed to us in a framework like this.
Things that we can gain or lose in our own lives, in the world, in societies. And you spin around so easily by how they're framed. Although now you know something. And you can think about them.
Here's just two odds and ends things, that I thought were kind of fun. Other things that people find kind of challenging to think their way through, just because of the way our minds are constructed.
So here's a problem. Jack is looking at Anne, but Anne is looking at George. Jack is married, but George is not.
Here's your problem to think about. Is a married person looking at an unmarried person, yes, no, or cannot be determined? The only reason I know the answers to these is because I have the answers to these. The first time I see these, I'm like, oh no.
So just for fun. You can go by intuition and maybe you worked it out or you looked ahead. How many people who haven't looked ahead, say yes? OK, there's some hands, 10.
How many say no? Nobody. How many say, cannot be determined? The answer is, that's good. Most answer, 80% say cannot be determined.
But the answer is this. Think about this. The answer is yes. And here's why. We don't know Anne's marital status. The other two are given. So we have to figure out Anne's situation.
Let's say she's married. Then she's looking at unmarried George. Let's say she's not married. Then Jack is looking at unmarried Anne. Is that OK?
This is what people in logic, call fully disjunctive reasoning, where if you work your way through the possibilities, you see it has to work out. But that's not our intuitions.
I mean you're a very smart group. So if it's hard for you, imagine the rest of the world is not as suspicious about how to figure their problems out.
So it's just fantastically interesting how the human mind is brilliant at some things. And for other things, it could solve, it could solve, intuitively, it's not often going to happen. We go by these shortcuts where either we don't get the answer at all or we come up with a completely wrong one, because of availability or framing.
Here's a quick one. You'll get this fast. It's fun.
A ball and a bat cost $1.10 in total. The bat cost a dollar more than the ball. How much did the ball cost? All right, you guys are now, you're very suspicious. You're thinking it through.
Most people love $0.10. But you can tell that's not going to work? Because then the bat would have to cost $1.10 and the total would be $1.20.
But a lot you were pulled to, most of us to, having a $1.10, because we don't quite think it through. There's $1.00. There's $0.10. And we do these shortcuts, these heuristics, instead of logically working our way through.
One more example of this kind of thing. Because now we're going to add one more element just for this.
Imagine that the US Department of Transportation has found that a particular German car is eight times more likely than a typical family car to kill occupants of another car in a crash. The federal government is considering restricting sale and use of this German car. Do you think sales of the German car should be banned in the US or do you think the German car should be banned from being driven on the US streets?
So here's one, given in this study to US participants. Or to do exactly the same thing, but it's not a German car. It's a Ford Explorer.
So American participants, if it's a German car, 73%, get that car off the road. If it's a Ford Explorer, sorry. If a German car, off the streets, you get 73%, If it's the American car, and Germany is making the decision, only 40% of the people think that Germany should do that.
Now, that's not purely logic. What is that? That's bias, right?
That's like, yeah, on my streets, we don't want these threatening machines from overseas. Germany, we've got to keep that economy going. Those Ford workers are really doing a good job.
So of course most decisions aren't hyperrational in the way we've been talking about, unknown people. Of course they involve things in the world that we already have feelings about. So you add that in and you can see that making a truly logical judgment is shockingly hard, if anything is a little complicated.

[[Frontal Lobes and Thinking]]
So let me turn for the last little bit to what we think of as some of the core parts of the brain for higher level thought, the struggle we have between the best we have in our mind and the traps we can fall into. And when we talk about that, we tend to focus on sort of three areas of the frontal lobe.
So let me just remind you of this. So here's the frontal lobes. And you could think of the frontal lobe I think as the business end of the brain. Here comes visual information coming in, auditory information coming in, touch coming in. Somewhere back here, would be taste.
So that's getting stuck in and figuring out what's out there. So the front half is the business end of the brain, acting upon the world. Moving your hands, moving your body, making decisions about what you do. Motor cortex, premotor cortex, a guy's motor cortex.
This so-called lateral prefrontal cortex, that's higher levels of thought. And this bottom part of ventromedial or orbitofrontal cortex, that we talked about with Phineas Gage.
So Oliver Sacks, in Chapter 13, talks about one example of a woman who has a large tumor in this ventromedial area. So sometimes I think there's a Roman philosopher who said that life is composed of desires and decisions, desires and decisions. So this part of the brain is involved in desires and this part in decisions. And I'll show that, roughly speaking.
So here's a woman who is a former research chemist. And she has a rapid personality change, becoming facetious, impulsive, superficial. And she a cerebral tumor in this ventromedial part of the brain.
When I see her, she's full of quips and cracks, often clever and funny. Yes, father, she says to me on one occasion; yes, sister, on another; yes, doctor, on a third.
What am I? And then she make some jokes. So she's constantly joking.
And she says, everything means nothing. Nothing at all, she says with a bright smile and a tone of one who makes a joke, wins an argument, or wins at poker. So she just finds that sort of there are facts of the world, but they just don't feel like they mean anything.
Because a tumor is influencing a part of the brain that we understand to organize the relationship between thoughts and feelings, between thoughts and feelings. And that's a huge way about how we go about the world, what we think, what we feel.
We've put it into a formula of some sense. And that's how we act upon the world, whether we help somebody, or don't help somebody, like somebody, or don't like somebody. All those sorts of things.
So when we think about behavior, we normally think of the challenge of getting it going. You get in your car and you turn on and you accelerate from 0 to 60. But a really interesting thing about the human brain is that we have lots of habits, things we acquire about how to deal with the world. So just as big a problem for our brains is stopping the course of action that's no longer relevant.
So the frontal cortex both has the job of initiating things, but also stopping things. And you can see it in a couple of ways, I'll show you.
So here's an example of a patient with a frontal lobe injury. They're drawn this scratchily, by a physician. And here's their exact copy.
Do you see they can't stop? Here's again the model, scratched out by the physician. Look at the patient. The patient can't stop.
So the problem's not starting. The problem is stopping the mental operation once it's going and repeating. And it's hard to put on the mental breaks.
Here's an example of a behavioral test of that kind. And I remember this, when I was testing patients in E17 and E18 here, I did this a fair bit. It's called the Wisconsin Card Sorting Test.
And what happens is, you put on four cards like these. These are the key cards. It's very mysterious when you give the test. You just put them down.
And you have a pile of cards over here. And you tell the person, your job is to pick a card from this pile and put it below one of these cards. And I'll tell you if you're right or wrong. And then use that information to know where to put the next card.
So imagine you are doing this experiment, and if you're color blind, there's an extra issue. I'll just warn you. But you get this first card. Where would you put it below? Where could you put it below?
AUDIENCE: [INAUDIBLE].
PROFESSOR: OK. That has an equal number. That's one possibility. What's another possibility, as the rule by which you might sort the card?
AUDIENCE: Color.
PROFESSOR: Color. Yeah, that's green. That goes with that.
What's another possibility that you can see? Shape. You can say that's a plus sign, that's a plus sign. They all seem reasonable.
So people usually pick one of those three. And if they happen to pick the one that I as a tester know from my instruction sheet, it's my job to reinforce. Let's pretend it starts with color.
If they do this, I go wrong. I go, hmm. And they pick another card and they try something else. They pick another card and they try something else. And after three or four cards, people pretty much get the color.
And then you go get another card out. You go right, right, right, right right, wrong. And when I did this, I was approximately your age. And most of the patients I tested were older.
And they go like, sonny, look back on your score sheet because it's correct. And I go no ma'am, no sir. I'm sorry about that. But it's wrong.
Because what you do is after 10 consecutive correct responses, you switch the rule from color to shape. After 10 correct shape, you switch to number. And then you do that again.
So you switch the rule after 10. So it's a test of mental flexibility, in a fairly simple way. Does that make sense?
They have to figure out what the answer is. Then they're zooming along. And you go, no stop. And you have to come up with the next answer.
So here's what happens for patients with frontal lobe lesions, they make a huge amount of what's called perseverative errors. So this is now the intellectual analog of the too many squiggles.
Once they get that first rule, they can't stop that rule. So they go color, color, color, color, color. They get about 10.
So you switch. And they just keep doing it. You go no, wrong, wrong, wrong, wrong, wrong. But they can't stop the rule that worked in the first place, without the prefrontal cortex being intact.
So our mental flexibility seems to depend tremendously on prefrontal cortex. And you can do the same kind of thing in imaging, just to make sure it's not just a patient thing. And again, when you have to do this very same kind of task, typical people activate prefrontal cortex to do the mental manipulation of the simple problem solving task.
So we talked about perserveration and its hard to stop and mental flexibility. With ventromedial lesions, you get some weird things. You get what they call utilization behaviors.
So again, we're having this idea what's the good thing about having very powerful habits? Is it very good to have very powerful habits, and we use a different word for this, when you read? Yeah. Because that's how you're a fast reader. You know how to read words really fast.
Is it good to have very powerful habits when you walk? Yeah. You don't want to go like, OK, I'm moving the leg now. Everybody get ready. We're moving the other leg. Because you could never get out of the room very efficiently.
So for many, many things, having very powerful habits are very strong. But for other things, they're not as good and you have to put the brakes on.
So Lhermitte described patients with ventromedial lesions who came in, saw in his office. This was not a set-up experiment. He didn't expect this. There was a hammer, a nail, and a picture. And the patient just goes over, up.
And you could imagine, if you go into the doctor's office and you see a hammer, a nail, and a picture, would you go over and put up the picture? No.
I mean you know that's what you do with that stuff. You know that's what's going on. But you go, that's not my place to do it, even though it's attempted to do it. The patient could not help himself but go do that, literally go do that.
The most dramatic example was, the patient walks in and sees a hypodermic needle, pulls down his pants, and injects himself in the derriere. So again, he knew that was going to happen. He knew that's the habit. But you wouldn't go into a doctor's office and start using their stuff and perform on yourself. You understand.
The habits are completely running the show. And any kind of frontal cortical control of those habits has been eliminated in these patients.
And here's a couple more things that are along that line. So I need a volunteer, in your seats, to try something with me for a minute. It's not bad. I think I've traumatized you in these examples. OK, thank you.
If you had to answer this question, and there's no correct answer. But let's just talk about how you might do it. How long is a man's spine, on average?
AUDIENCE: About two feet?
PROFESSOR: And how did you kind of come to that? And that might be right or not.
AUDIENCE: My back is roughly two feet long. My spine is--
PROFESSOR: Yeah. It's got to be less seven feet. Because that's like the tallest basketball players. On average, it's got to be more than a couple feet. Something in that range. OK, you just guessed which it was.
Or how fast does a horse in a race gallop? Or what's the largest object normally found in the house? What do you think that might be?
AUDIENCE: Fridge?
PROFESSOR: Help me out here.
AUDIENCE: A refrigerator.
PROFESSOR: A refrigerator is a good one. Anybody else?
AUDIENCE: [INAUDIBLE].
PROFESSOR: All those are good. Is toaster a good answer? No.
One more person. Who's going to help me out here? One more example. Come on. One more example. It's easy. OK, thank you, Rich.
How many camels are there in the Netherlands? Now, nobody knows. But tell me a thought process roughly, so you would come up with an estimate.
AUDIENCE: 100.
PROFESSOR: Yeah. Because are you going to figure there's a lot in the Netherlands? No. Because there's no desert, right? Where might you find a few?
AUDIENCE: [INAUDIBLE]
PROFESSOR: On a ranch. I haven't been to Amsterdam for a while, so perhaps there's a few loping around in the canals. And some zoo, some kids' petting thing, who knows? So, yeah, 100 sounds good to me.
But frontal patients will give really bad answers, thousands or zero. Or 12 feet for a spinal cord. Or just something that you go like, where did you come up that?
You can do the Price Is Right game with them. They'll come up with more ridiculous answers. Here's just a graph. These are bad answers for how expensive something is, just unreasonable answers within these sort of open-ended questions.
Or you can do more formal problems. The kind of problem where you're given a puzzle like this. And your job is to move these into a target state.
And so to get over to here, you've got to move the blue guy over here and whatever, in a different number of moves. That's planning ahead and problem solving. Patients with frontal lobe lesions make many mistakes on these kinds of tasks.
So the last thing I want to talk is a little bit about Phineas Gage kind of lesions. And we'll get in a moment to a comment about serial killers. So Phineas Gage had this huge injury. We talked about him already in the ventromedial prefrontal cortex.
There's some other patients who have orbitofrontal lesions, who have been studied. And let me tell you a little bit about what's known about them. And I can tell you this is the part of the brain that's been most implicated in psychopaths, some of whom turn out to be these serial killers.
One measure they've used in these experiments is galvanic skin response. That's the kind of stuff they use in polygraphs, lie detectors. They put them on your skin and what they measure is responses that are related to sweat glands. So it's a measure of emotional reactivity, basically.
Here's a really interesting thing. If we measure your galvanic skin response and we show you faces of people you know, family members, friends, and so on, versus people you don't know, you get a galvanic skin response for the people you know, because they matter to you. It's an emotional response, versus faces you don't know.
You even get that kind of strikingly in several studies in patients with prosopagnosia. So we talked about those patients who can't recognize people by their faces. But something in them still recognizes who's familiar and who's not. Even if they can't tell you, that's my mother or that's my father or that's my brother, their galvanic skin response is a pretty good response.
Patients with orbitofrontal lesions are exactly the opposite. When they're shown pictures of family members and friends, they have no more of a response in the sweat glands, in the autonomic nervous system, than they do for absolute strangers. And you might intuitively imagine that it's pretty easy to lie and deceive your friends and family, if they don't mean to you anything more than an absolute stranger.
No galvanic response to emotional versus neutral pictures. And so these patients seem to have this disconnection between thought and emotion. They know who the person is. But they don't have an emotional response of any kind to that person.
And here's one clever experiment that shows some of the consequence of that. So this is work from Damasio and his colleagues. So they took a couple of these patients. There's not that many of them. And they put them into an experiment where you could pick from two decks.
One deck has a low immediate reward, but positive long-term rewards. It's a little bit like studying a lot, and hoping it all pays off. And another one is high immediate rewards, but higher long-term losses. Like not studying has some unpleasant consequences.
So two decks of cards. And you don't know this, but you just quickly discover that Pile A, you get the occasional $50, but you often lose $100. But Pile B, you get the thrill of the $100 loss, but you often lose $1,000. So they're kind of pitting against each other, the thrill of the $100 win, versus the certain doom of the quick thrill pile. You get the quick thrill, but it will doom you.
So what do typical people do? After a little while, they figure out well, Pile B is pretty fun when I get the $100. But it beats me up a lot in the long term, so they go for Pile A.
Orbitofrontal patients don't. They go for that one-time thrill, over any long-term consequences.
As interesting as that, if you measure the galvanic skin response, for control people, typical people, when they go for the risky pile, their heart is pounding, their hands are sweating. And they go, I'm going to walk on the wild side. Back to the save file, mostly.
No galvanic skin response for the patients with orbitofrontal lesions. Yeah?
AUDIENCE: Do people get the lesions after reaching adulthood?
PROFESSOR: So this is interesting. I'm going to come to this in a moment. Actually literally, absolutely right now. Same pattern for people who have these injuries at 15 months or three months, as happens in adulthood.
Now, we don't know if that's everybody. But these are patients who came forth with problems. So this is a little bit like, we don't know if there are a lot of people with damage there, who are not doing this.
But these two patients who they studied, had that. And they got in lots of trouble in school, all the time. They did all the things you might imagine a three-year-old, and for the rest of us, like Phineas Gage would do.
So that raises the question about whether the kinds of people who behave in this most disturbing, psychopathic, serial killer fashion have either an injury or something like that, that makes them predisposed to being very inhumane in the way they think about people.
And just in case you, as careful MIT scientists, worry about well, maybe they're not having a GSR that works at all, what does that mean? If these patients who had the damage early in childhood just get a loud tone, a big boat horn, like ooh, there's a galvanic skin response that looks pretty good.
So it's not their autonomic system won't respond to emotionally provocative things, like a big sound. It just doesn't respond to emotionally provocative things like risk and people you should care about.
And I'll just end in this one minute. So this is an area where a guy, Kent Khiel, at the University of New Mexico. You can go look this up. It's an amazing story. He's very interested in these brain differences in psychopaths in prison. Many are in prison because they're scary people.
So you can't get them out to do research very easily. So he has a truck with an MR scanner. And he drives from penitentiary to penitentiary and does brain scanning with these people. And is getting on average, some differences in this brain region.
So I have a question for you, just for one minute. Do you think that should be introduced in a court case for a serial killer? If he gets up there and shows you brain pictures, should a jury hear about that or should they not hear about that, for somebody who's done an awful murder or an awful string murders?
AUDIENCE: I'd say no.
PROFESSOR: Sorry. You say no. Because what?
AUDIENCE: I think that the risk is too great that you will have innocent people be shown. And the science is not fully resolved yet in that respect. There are also a lot of salient factors and environmental factors that have been constrained by genetics, and say, oh, if you have this huge issue, this condition of the brain, you're automatically [INAUDIBLE].
PROFESSOR: OK. Let me tell one sentence. Yeah, go ahead.
AUDIENCE: You know the correlation in one direction--
PROFESSOR: We don't know. These are good points. Let me just say, so here's where we stand now, because this will be your lives in the future, in a lot of different ways.
Where we stand now is, for the first time ever, brain imaging evidence was introduced in the court case about a year ago of a serial killer. They were only allowed to do it-- court cases like this go through two phases. You see these in TV shows or movies.
There's the initial trial. And if the person is found guilty, there's a second phase where they decide what the appropriate penalty is. The judge said they could only be reported in the penalty phase, when people were thinking about what's the right punishment for a person, not before the conviction.
And they said they couldn't show any color pictures of brain stuff, because that would over-influence the jurors. And so you could only talk about this stuff. Because if you showed a brain picture, sort of like you said, it would look so scientific that they were afraid it overwhelm the jurors' judgment.
So this is all coming up, about what the brain maybe do in your lifetimes. Thanks.